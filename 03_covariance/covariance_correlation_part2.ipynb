{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Path to Empirical Orthogonal Function Analyses\n",
    "\n",
    "Today we're going to continue to set the stage to use empirical orthogonal function (EOF) analysis (or principal components analysis, PCA) on our spatiotemporal data.  We'll learn about these in the next lecture and they will be our new friend -- in one form or another -- through the rest of the class.  \n",
    "\n",
    "Recall the we anticipate that our data will be linearly dependent - that some of our time series can be expressed as linear combinations of others.  This means they will covary, they will be correlated, and they will not be orthogonal (they will have a non-zero dot product when the time series of our data and multiplied with one another).  As we've said, this both provides challenges (mathematically and statistically) but also potentially reveals something about the larger-scale and perhaps causal nature of our data.  EOF analysis and the related techniques we will learn (and still others that we don't have time for) will allow us to both handle the challenge and leverage the advantages of our data. \n",
    "\n",
    "Before we get started, let's import NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Matrices\n",
    "\n",
    "Let's revisit some of the special matrices we learned about in linear algebra.  The first is the _**square matrices**_, which is simply those matrices with the same number of rows and columns.  Another (related) special matrix is the _**symmetrical matrices**_, where the following holds true:\n",
    "\n",
    "$\n",
    "\\mathbf{A} = \\mathbf{A}^T \n",
    "$\n",
    "\n",
    "Or, a symmetrical matrix is a matrix that is equal to its transpose.  It is easy to see why a symmetrical matrix must also be a square matrix (how could a matrix with different numbers of rows and columns be equal to its transpose, which swaps columns for rows?).  One of the other special matrices we learned about -- the identity matrix -- is a square symmetric matrix with 1 on the diagonal and 0 everywhere else:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.identity(4) # can also use np.eye(4)\n",
    "print(I)\n",
    "I == I.T # all entries in this equality comparison are true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember you can get the diagonal elements of a matrix using `np.diag`, `np.diagonal`, or `.diagonal()` (which uses the object-oriented nature of Python), and you can get the sum of the diagonal elements (called the **trace**) using `np.trace` or `.trace()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various ways of getting the diagonal elements\n",
    "print(np.diag(I))\n",
    "print(np.diagonal(I))\n",
    "print(I.diagonal())\n",
    "\n",
    "# getting the trace, the sum of the diagonal elements:\n",
    "print(np.trace(I))\n",
    "print(I.trace())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compatible matrices and matrix multiplication\n",
    "\n",
    "By now you're probably quite tired of hearing me talk about matrix multiplication!  But being comfortable with it is key to building your intuition and ability to do EOF analyses using basic matrix operations.  Let's briefly review some of the most important points.\n",
    "\n",
    "We can multiply matrices so long as the number of columns in the first matrix is the same as the number of rows in the second matrix.  Or, in otherwords, their _**inner dimensions**_ are the same.  In this case we can calculate the dot or inner product of two matrices or vectors.  Let's look at a simple example:\n",
    "\n",
    "$\n",
    "\\mathbf{A} = [2\\: {-1}\\: 6\\: 3]\n",
    "$\n",
    "\n",
    "Here, **A** is a vector with 1 row and 4 columns, or a 1x4 matrix (note that Python allows you to have 1-dimensional arrays, so a Python element can simply have 4 elements and not necessarily be row-vector or column-vector).  If we find the product:\n",
    "\n",
    "$\n",
    "\\mathbf{A}\\mathbf{A}^T\n",
    "$\n",
    "\n",
    "We can see that this would be the multiplication of a 1x4 vector with a 4x1 vector.  Since the inner dimensions are both 4, we can calculate the dot product, and since the outer dimensions are 1 and 1, we expect the outcome of this dot product to have a dimension of 1x1 -- or, a scalar:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([2, -1, 6, 3]);\n",
    "print(A.shape) # Python allows one dimensional arrays(!), so to demonstrate what I'm talking about here, I ...\n",
    "\n",
    "# Add another dimension to A so that it is truly (1,4), not (4,), so that np.dot will work as expected\n",
    "A = A[np.newaxis,:]\n",
    "print(A.shape) # A is now 2-dimensional, with 1 row and 4 columns\n",
    "\n",
    "np.dot(A,A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the dot product is a scalar equal to 50, which comes from the sum of the products of each element: (2x2) + (-1x-1) + (6x6) + (3x3) = 4 + 1 + 26 + 9 = 50.\n",
    "\n",
    "What if we did it the otherway, though?  Remember, in matrix algebra multiplication is not commutative.  What if we calculate the dot product of:\n",
    "\n",
    "$\n",
    "\\mathbf{A}^T\\mathbf{A}\n",
    "$\n",
    "\n",
    "Now we would have a 4x1 matrix and a 1x4 matrix -- with the inner dimension of 1, the matrices are compatible -- they can be multiplied -- but now we see that the outer dimensions lead us to expect a 4x4 matrix!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.dot(A.T,A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these entries?  Take a moment and realize that (1) each row and column position in the answer is the simple product of those positions in A (e.g. row 1, column 2 is the product of position 1 and position 2), and then (2) that the diagonal of the matrix contains the individual products that lead to the scalar answer above. \n",
    "\n",
    "What you might also notice is that this is a square symmetric array:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(B)\n",
    "print(B.T)\n",
    "B == B.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extend this to a simple 2-dimensional matrix. Consider the following matrix:\n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix} 4 &   -2  &  12  \\\\ -2   &  1  &  -6  \\\\ 12  &  -6   & 36  \\\\ -6  &   3  & -18   \\end{bmatrix} \n",
    "$\n",
    "\n",
    "We might imagine this matrix reflects a subset of our spatiotemporal data, with 4 years of observations (the rows) at 3 locations (the columns).  We know we can find the dot product if the inner dimensions are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4, -2, 12],[-2, 1, -6],[12, -6, 36],[-6, 3, -18]])\n",
    "C = np.dot(A.T,A) # at 3x4 matrix times a 4x3 matrix gives us a 3x3 matrix\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, notice that the result is a square symmetric matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C==C.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger matrices, looking at a screen full of 'True' or 'False' might not be that efficient (or pleasant), so you could use the [`.all()` method](https://docs.python.org/3/library/functions.html#all) appended to the equality to get a single answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(C==C.T).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance (and correlation) matrices\n",
    "\n",
    "Finally, let's look at covariance matrices as a special case of a symmetric square matrix that contains information about our spatiotemporal data.  Let's return to our matrix above, but thinking this time specifically about covariance.  \n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix} 4 &   -2  &  12  \\\\ -2   &  1  &  -6  \\\\ 12  &  -6   & 36  \\\\ -6  &   3  & -18   \\end{bmatrix} \n",
    "$\n",
    "\n",
    "Let is again imagine this matrix reflects a subset of our spatiotemporal data, with 4 years of observations (the rows) at 3 locations (the columns).  How would we calculate the covariance using just matrix operations?  First, we'll need to remove the mean (since variance is a measure of dispersion around a mean value) and we need to divide by the number of samples less one (for it to be unbiased).  \n",
    "\n",
    "Let's imagine we want specifically to know the relationship through time of the phenomena we are measuring at each of our locations.  Or, in otherwords, we want to know how much each site goes 'up and down' together through time - are they in sync? Out of sync?  Not related at all? \n",
    "\n",
    "Let's start out by removing the mean value from each location (each column, in this matrix).  Remember in the calculation of covariance between 2 vectors (or time series) we are finding the products of the values with the mean subtracted:\n",
    "\n",
    "$\n",
    "cov(x,y) = \\frac{1}{n-1}\\sum_{i}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\n",
    "$\n",
    "\n",
    "To do this efficiently, Python allows us to do something called broadcasting, which describes how NumPy treats arrays with different shapes when doing arithmetic operations.  In this case, we can subtract the mean of each column from the data matrix to get the centered or anomaly values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_bar = np.mean(A,axis=0)\n",
    "print(A_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that to calculate the mean, we had to specify the `axis` in the call to `np.mean` (otherwise, NumPy defaults to flattening the array and just calculating the mean of all the values as if they were in a single long list or strung-out-vector). \n",
    "\n",
    "If we want to remove the mean from the values in each column, we can simply do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A - np.mean(A,axis=0)) # subtract the mean of the columns from the respective columns\n",
    "print(np.mean(A - np.mean(A,axis=0),axis=0)) # check - mean of each column should now be zero! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operation of subtracting a 1x3 array from a 4x3 array works because Python has a set of rules to broadcast unmatched dimensions when performing an operation.  See here for a description of this within NumPy: https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "\n",
    "The next operation we would need to do to calculate covariance using matrix operations would be the matrix multiplication step.  Remember that this part of the covariance equation:\n",
    "\n",
    "$\n",
    "\\sum_{i}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\n",
    "$\n",
    "\n",
    "This says 'take the sum of the products of the corresponding values of vector _x_ and _y_ with their mean removed'.\n",
    "\n",
    "Let's see if we can first generalize the above equation (for the covariance of two vectors) to instead indicate the covariance of _each column_ of a data matrix **A** of size $n$ rows and $p$ columns, $a_1$, $a_2$, $a_3$, ... $a_p$:\n",
    "\n",
    "$\n",
    "\\mathbf{A} = \n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} & \\dotsm & a_{1,p}\\\\\n",
    "a_{2,1} & a_{2,2} & \\dotsm & a_{2,p}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{n,1} & a_{n,2} & \\dotsm & a_{n,p}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "cov(a_j,a_k) = \\frac{1}{n-1}\\sum_{i}^{n} (a_{ij} - \\bar{a_j})(a_{ik} - \\bar{a_k})\n",
    "$\n",
    "\n",
    "Or, the covariance of any two columns $a_j$ and $a_k$ in the matrix **A** is the sum of the products of the elements of each row $n$ with the mean of that column ($\\bar{a_j}$ or $ \\bar{a_k}$, respectively) removed).  We may recoginize that when $j==k$ we are simply calculating the variance of that column! \n",
    "\n",
    "The covariance matrix is therefore a **square symmetrical matrix** of size _p_ x _p_ describing the covariance of each column of **A** (1 thought _p_) with all the other columns of **A** (1 through _p_).  We have therefore generalize our two vector (or time series) approach to consider the covariance between all possible pairs of the columns of **A**. \n",
    "\n",
    "This can be written as:\n",
    "\n",
    "$\n",
    "\\mathbf{C} = \n",
    "\\begin{bmatrix}\n",
    "cov(a_{1},a_{1}) & cov(a_{1},a_{2}) & \\dotsm & cov(a_{1},a_{p})\\\\\n",
    "cov(a_{2},a_{1}) & cov(a_{2},a_{2}) & \\dotsm & cov(a_{2},a_{p})\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "cov(a_{p},a_{1}) & cov(a_{p},a_{2}) & \\dotsm & cov(a_{p},a_{p})\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "or, since the covariance of one column of **A** with itself is the same as the variance of that column:\n",
    "\n",
    "$\n",
    "\\mathbf{C} = \n",
    "\\begin{bmatrix}\n",
    "var(a_{1}) & cov(a_{1},a_{2}) & \\dotsm & cov(a_{1},a_{p})\\\\\n",
    "cov(a_{2},a_{1}) & var(a_{2}) & \\dotsm & cov(a_{2},a_{p})\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "cov(a_{p},a_{1}) & cov(a_{p},a_{2}) & \\dotsm & var(a_{p})\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Observe a few things, some of which are review:  First, the diagonal elements contain the variances of each column ($a_1$, $a_2$, $a_3$, ... $a_p$).  Next, the matrix will be **square**, with $p$ rows and $p$ columns.  Third, the matrix is **symmetrical** (e.g. the value in row 1, column 2 is the same as the value in row 2, column 1).  \n",
    "\n",
    "What this means is that we have a special matrix (square, symmetrical) that contains all the information about how all the columns of our data matrix relate to one another!\n",
    "\n",
    "Now, let's finish the matrix operations.  Now, we could go column by column and calculate the covariance of each individually ($a_1$ with $a_2$, $a_1$ with $a_3$, $a_2$ with $a_1$, etc. etc.), _**but**_ we should also be able to see by now that matrix multiplication allows us to do this much quicker!  Since matrix multiplication is the sum of the products of each row of a matrix with the corresponding column in another, if we multiply the transpose of the centered (mean removed, or zero-mean) matrix **A0** by the non-tranposed version of the same matrix, we will in doing so calculate the sum of the products of each column of the data matrix with itself!  As well, if we take the transpose of our $n$ x $p$ matrix -- $p$ x $n$ and multiply it by the original $n$ by $p$ matrix, the inner dimensions (the number of rows in our data matrix) match and the outer dimensions (the number of columns in our data matrix) indicate the matrix product will be $p$ x $p$.  All we need to do is divide by $n-1$ (where $n$ is the number of rows or observations) and we've got a covariance matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A0 = A - A.mean(axis=0) # we'll call the centered matrix A0 ('A naught'), it is still 4x3\n",
    "print(A0)\n",
    "1/(A0.shape[0]-1) * (A0.T @ A0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there is an easier way - NumPy offers `np.cov` which can do the matrix calculation for us.  _**A strong warning**_, though.  [As the `np.cov` documentation says](https://numpy.org/doc/stable/reference/generated/numpy.cov.html), for the input matrix the function assumes that 'each row represents a variable, and each column a single observation of all those variables' -- this is in fact the opposite of how we've thus far envisioned our data matrix in the notebook above and the opposite of how MATLAB, R, and even [`Pandas` in Python](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cov.html) does it!  \n",
    "\n",
    "To get the answer we got above, where we calculate the covariance between the columns of our data matrix, and the rows are observations of the variables or locations in each column, we need to invert the centered matrix first in the call to `np.cov`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(A0.T,ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `np.cov` knows to remove the mean, so we get the same answer if we use the transpose of the data matrix before we removed the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.cov(A.T,ddof=1)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to emphasize my **warning** -- since dimensionality and how different programming languages and libraries and packages make assumptions about dimensionality varies in very important ways, always be extremely careful when calculating covariance matrices (or using any built-in operators) - this is why we've spent some time building a background in these calculations, so that you have an intuition about what to expect and how to identify if something is happening in a different way then you intend.\n",
    "\n",
    "Let's look at the elements of the covariance matrix above.  It is indeed 3 by 3 and so square. We expect to find that the diagonal elements of the covariance matrix are the variance of the original data columns and they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.diagonal(C))\n",
    "print(np.var(A,axis=0,ddof=1)) # specify to take the variance down columns and us the unbiased estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also see that the matrix is symmetrical - the off-diagonal elements are a mirror image of each other.\n",
    "\n",
    "The covariance matrix encapsulates the similarity (or not) between the columns of the data matrix **A** as well as the total variance in each column of data.   It seems from the large-ish positive value in row 3, column 1 and in row 1, column 3 of the covariance matrix that column 1 and column 3 of our data matrix are similar, although it is tough to put that number into relative terms - how similar?  Likewise, negative values in both row 2, column 1, and row 1, column 2 as well as row 2, column 3, and row 3, column 2 indicate that the data in column 2  of **A** is negatively covarying with both column 1 and column 3 -- when column 1 and 3 go up, column 2 goes down, and vice versa.  \n",
    "\n",
    "As we've already seen, the correlation matrix is similar to the covariance matrix but is normalized by the variance, meaning correlation coefficients will be between -1 and 1 and therefore more readily interpretable.  Recall the correlation is calculated as: \n",
    "\n",
    "$\n",
    "corr(x,y) = r_{x,y} = \\frac{1}{n-1}\\sum_{i=1}^n  \\Big( \\frac{x_i - \\bar{x}}{\\sigma_x} \\Big)  \\Big( \\frac{y_i - \\bar{y}}{\\sigma_y} \\Big) \n",
    "$\n",
    "\n",
    "Based on how we calculated the covariance matrix above using only matrix operations, we might see that we could do the same for correlation if we normalized (divided by) the variance of each column of the data.\n",
    "\n",
    "Another way to look at this would be that we cannot only remove the mean of each column before calculating their covariance, but also set each column to have the same variance!   We do this by dividing each column of number by the standard deviation of each column _after_ we first remove the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = (A - A.mean(axis=0)) / np.std(A,axis=0,ddof=1)\n",
    "print(np.std(Z,axis=0,ddof=1)) # will reveal that the standard deviation of each column is now 1\n",
    "print(np.var(Z,axis=0,ddof=1)) # will reveal that the variance of each column is now 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a matrix **Z** that has column means of 0 and column standard deviations (and variances) of 1, we can do our correlation calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 1/(Z.shape[0]-1) * (Z.T @ Z)\n",
    "print(R)\n",
    "print(np.corrcoef(Z.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh!  Look at that.  The correlation matrix indicates that all the columns of the matrix **A** we've been using are perfectly correlation (column 1 and column 3) or perfectly anti-correlation (column 2 with either column 1 or column 3).  So our intuition about their relationship from their covariance was correct, but the correlation matrix more readily reveals exactly how strongly they covaried! \n",
    "\n",
    "Let's take another look at that data matrix:\n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix} 4 &   -2  &  12  \\\\ -2   &  1  &  -6  \\\\ 12  &  -6   & 36  \\\\ -6  &   3  & -18   \\end{bmatrix} \n",
    "$\n",
    "\n",
    "With another look, it quickly becomes obvious that our column 2 is just a scaled (by -0.5) version of column 1.  And column 3 is similarly just a scaled (by 3) version of column 1 as well.   Our matrix is strongly linearly dependent!\n",
    "\n",
    "Remember the `rref()` function in `sympy`?  In addition to finding the 'reduced row echelon form', we can also ask it to return the independent columns of A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy \n",
    "ref, inds = sympy.Matrix(A).T.rref() # transpose because we're looking at the covariance of each column here \n",
    "print(ref)\n",
    "print(inds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice a few things.  In the RREF returned by the code, the 2nd and 3rd rows are all zero, which we learned is an indication of 'free variables' (that $x_2$ and $x_3$ could take any value, or there are infinitely many solutions).  The second output, `inds` tell us that only the 0th column is independent (or, really, that all the columns are a variant of column 1).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is the case, we might expect to find that the data matrix is rank deficit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(A) # only rank 1, even though A is 4x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and that the condition number will not be close to 1 - it may be very large or even infinite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cond(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our data matrix is rank deficit and ill-conditioned, as we might expect.   However, as we'll soon see, the covariance (or correlation matrix) of a rank deficit, linearly dependent, and ill-conditioned matrix offers a way forward toward dealing with the covariance and linear dependence in our real data systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
